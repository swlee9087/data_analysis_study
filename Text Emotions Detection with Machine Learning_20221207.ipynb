{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8859afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REF https://thecleverprogrammer.com/2021/02/19/text-emotions-detection-with-machine-learning/\n",
    "# REF https://thecleverprogrammer.com/2021/03/21/end-to-end-machine-learning-project/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a14b7b97",
   "metadata": {},
   "source": [
    "https://github.com/amankharwal/Website-data/raw/master/text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fcbd704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a24bf704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 7480\n"
     ]
    }
   ],
   "source": [
    "def read_data(file):\n",
    "    data = []\n",
    "    with open(file, 'r')as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            label = ' '.join(line[1:line.find(\"]\")].strip().split())\n",
    "            text = line[line.find(\"]\")+1:].strip()\n",
    "            data.append([label, text])\n",
    "    return data\n",
    "\n",
    "file = 'text.txt'\n",
    "data = read_data(file)\n",
    "print(\"Number of instances: {}\".format(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12fe7345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(token, n): \n",
    "    output = []\n",
    "    for i in range(n-1, len(token)): \n",
    "        ngram = ' '.join(token[i-n+1:i+1])\n",
    "        output.append(ngram) \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85fe71d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature(text, nrange=(1, 1)):\n",
    "    text_features = [] \n",
    "    text = text.lower() \n",
    "    text_alphanum = re.sub('[^a-z0-9#]', ' ', text)\n",
    "    for n in range(nrange[0], nrange[1]+1): \n",
    "        text_features += ngram(text_alphanum.split(), n)    \n",
    "    text_punc = re.sub('[a-z0-9]', ' ', text)\n",
    "    text_features += ngram(text_punc.split(), 1)\n",
    "    return Counter(text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81ca4cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(item, name): \n",
    "    items = list(map(float, item.split()))\n",
    "    label = \"\"\n",
    "    for idx in range(len(items)): \n",
    "        if items[idx] == 1: \n",
    "            label += name[idx] + \" \"\n",
    "    \n",
    "    return label.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77445d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = [\"joy\", 'fear', \"anger\", \"sadness\", \"disgust\", \"shame\", \"guilt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5043b13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = []\n",
    "y_all = []\n",
    "\n",
    "for label, text in data:\n",
    "    y_all.append(convert_label(label, emotions))\n",
    "    X_all.append(create_feature(text, nrange=(1, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "872101a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size = 0.2, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8aa76697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(clf, X_train, X_test, y_train, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_acc = accuracy_score(y_train, clf.predict(X_train))\n",
    "    test_acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "    return train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4f8a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vectorizer = DictVectorizer(sparse = True)\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a10864ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "lsvc = LinearSVC(random_state=123)\n",
    "rforest = RandomForestClassifier(random_state=123)\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "clifs = [svc, lsvc, rforest, dtree]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6b20ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "febe3509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Classifier                | Training Accuracy | Test Accuracy |\n",
      "| ------------------------- | ----------------- | ------------- |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                               | 1/4 [00:35<01:47, 35.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| SVC                       |         0.9067513 |     0.4512032 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\sd_env\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "\r",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                          | 2/4 [00:39<00:34, 17.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| LinearSVC                 |         0.9988302 |     0.5768717 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 3/4 [01:10<00:23, 23.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| RandomForestClassifier    |         0.9988302 |     0.5541444 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:20<00:00, 20.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| DecisionTreeClassifier    |         0.9988302 |     0.4618984 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train and test them \n",
    "print(\"| {:25} | {} | {} |\".format(\"Classifier\", \"Training Accuracy\", \"Test Accuracy\"))\n",
    "print(\"| {} | {} | {} |\".format(\"-\"*25, \"-\"*17, \"-\"*13))\n",
    "for clf in tqdm(clifs): \n",
    "    clf_name = clf.__class__.__name__\n",
    "    train_acc, test_acc = train_test(clf, X_train, X_test, y_train, y_test)\n",
    "    print(\"| {:25} | {:17.7f} | {:13.7f} |\".format(clf_name, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8e960cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joy       (1. 0. 0. 0. 0. 0. 0.)  1084\n",
      "anger     (0. 0. 1. 0. 0. 0. 0.)  1080\n",
      "sadness   (0. 0. 0. 1. 0. 0. 0.)  1079\n",
      "fear      (0. 1. 0. 0. 0. 0. 0.)  1078\n",
      "disgust   (0. 0. 0. 0. 1. 0. 0.)  1057\n",
      "guilt     (0. 0. 0. 0. 0. 0. 1.)  1057\n",
      "shame     (0. 0. 0. 0. 0. 1. 0.)  1045\n"
     ]
    }
   ],
   "source": [
    "l = [\"joy\", 'fear', \"anger\", \"sadness\", \"disgust\", \"shame\", \"guilt\"]\n",
    "l.sort()\n",
    "label_freq = {}\n",
    "for label, _ in data: \n",
    "    label_freq[label] = label_freq.get(label, 0) + 1\n",
    "\n",
    "# print the labels and their counts in sorted order \n",
    "for l in sorted(label_freq, key=label_freq.get, reverse=True):\n",
    "    print(\"{:10}({})  {}\".format(convert_label(l, emotions), l, label_freq[l]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edf1d146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This looks so impressive ðŸ˜³\n",
      "I have a fear of dogs ðŸ˜±\n",
      "My dog died yesterday ðŸ˜¢\n",
      "I don't love you anymore..! ðŸ˜‚\n"
     ]
    }
   ],
   "source": [
    "emoji_dict = {\"joy\":\"ðŸ˜‚\", \"fear\":\"ðŸ˜±\", \"anger\":\"ðŸ˜ \", \"sadness\":\"ðŸ˜¢\", \"disgust\":\"ðŸ˜’\", \"shame\":\"ðŸ˜³\", \"guilt\":\"ðŸ˜³\"}\n",
    "t1 = \"This looks so impressive\"\n",
    "t2 = \"I have a fear of dogs\"\n",
    "t3 = \"My dog died yesterday\"\n",
    "t4 = \"I don't love you anymore..!\"\n",
    "\n",
    "texts = [t1, t2, t3, t4]\n",
    "for text in texts:\n",
    "#     clf = dtree\n",
    "#     clf_name = clf.__class__.__name__\n",
    "#     print(clf_name)\n",
    "    features = create_feature(text, nrange=(1, 4))\n",
    "    features = vectorizer.transform(features)\n",
    "    prediction = clf.predict(features)[0]\n",
    "    print( text,emoji_dict[prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9954afe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 16)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "corpus = ['This is the first document.', \n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.', \n",
    "          'Is this the first document?']\n",
    "vectorizer = HashingVectorizer(n_features=2**4)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "522f08d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x16 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b35613f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t-0.5773502691896258\n",
      "  (0, 8)\t-0.5773502691896258\n",
      "  (0, 13)\t0.5773502691896258\n",
      "  (0, 14)\t0.0\n",
      "  (1, 0)\t-0.8164965809277261\n",
      "  (1, 11)\t0.4082482904638631\n",
      "  (1, 13)\t0.4082482904638631\n",
      "  (1, 14)\t0.0\n",
      "  (2, 4)\t-0.7071067811865475\n",
      "  (2, 5)\t0.7071067811865475\n",
      "  (2, 13)\t0.0\n",
      "  (2, 14)\t0.0\n",
      "  (3, 0)\t-0.5773502691896258\n",
      "  (3, 8)\t-0.5773502691896258\n",
      "  (3, 13)\t0.5773502691896258\n",
      "  (3, 14)\t0.0\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "616a3dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1, 2], [1, 4], [1, 0],\n",
    "              [10, 2], [10, 4], [10, 0]])\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45948e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.predict([[0, 0], [12, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d6188cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.,  2.],\n",
       "       [ 1.,  2.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f665125d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\sd_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 2048 or by setting the environment variable OMP_NUM_THREADS=1\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2. , 1. ],\n",
       "       [3.5, 4.5]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import numpy as np\n",
    "X = np.array([[1, 2], [1, 4], [1, 0],\n",
    "              [4, 2], [4, 0], [4, 4],\n",
    "              [4, 5], [0, 1], [2, 2],\n",
    "              [3, 2], [5, 5], [1, -1]])\n",
    "# manually fit on batches\n",
    "kmeans = MiniBatchKMeans(n_clusters=2,\n",
    "                         random_state=0,\n",
    "                         batch_size=6)\n",
    "kmeans = kmeans.partial_fit(X[0:6,:])\n",
    "kmeans = kmeans.partial_fit(X[6:12,:])\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76227434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.predict([[0, 0], [4, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d51896c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\sd_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 2048 or by setting the environment variable OMP_NUM_THREADS=1\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.19354839, 1.22580645],\n",
       "       [4.03125   , 2.46875   ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit on the whole data\n",
    "kmeans = MiniBatchKMeans(n_clusters=2,\n",
    "                         random_state=0,\n",
    "                         batch_size=6,\n",
    "                         max_iter=10).fit(X)\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab51128a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.predict([[0, 0], [4, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58c7150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd_env",
   "language": "python",
   "name": "sd_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
